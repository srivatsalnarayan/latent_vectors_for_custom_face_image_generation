{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10c0f78e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SRIVATSAL NARAYAN\\anaconda3\\envs\\myenv\\lib\\site-packages\\h5py\\__init__.py:36: UserWarning: h5py is running against HDF5 1.12.2 when it was built against 1.12.1, this may cause problems\n",
      "  _warn((\"h5py is running against HDF5 {0} when it was built against {1}, \"\n"
     ]
    }
   ],
   "source": [
    "# Import the required libraries\n",
    "from numpy import zeros, ones\n",
    "from numpy.random import randn, randint\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Reshape, Flatten, Conv2D, Conv2DTranspose, LeakyReLU, Dropout\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "caec66ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the standalone discriminator model\n",
    "# Input would be 128x128x3 images and the output would be a binary (using sigmoid)\n",
    "#Remember that the discriminator is just a binary classifier for true/fake images.\n",
    "def define_discriminator(in_shape=(128,128,3)):\n",
    "\tmodel = Sequential()\n",
    "\t# normal\n",
    "\tmodel.add(Conv2D(128, (3,3), padding='same', input_shape=in_shape))\n",
    "\tmodel.add(LeakyReLU(alpha=0.2))\n",
    "\t# downsample to 64x64\n",
    "\tmodel.add(Conv2D(128, (3,3), strides=(2,2), padding='same'))\n",
    "\tmodel.add(LeakyReLU(alpha=0.2))\n",
    "\t# downsample to 32x32\n",
    "\tmodel.add(Conv2D(128, (3,3), strides=(2,2), padding='same'))\n",
    "\tmodel.add(LeakyReLU(alpha=0.2))\n",
    "\t# downsample to 16x16\n",
    "\tmodel.add(Conv2D(128, (3,3), strides=(2,2), padding='same'))\n",
    "\tmodel.add(LeakyReLU(alpha=0.2))\n",
    "\t# downsample to 8x8\n",
    "\tmodel.add(Conv2D(128, (3,3), strides=(2,2), padding='same'))\n",
    "\tmodel.add(LeakyReLU(alpha=0.2))\n",
    "\t# classifier\n",
    "\tmodel.add(Flatten())\n",
    "\tmodel.add(Dropout(0.4))\n",
    "\tmodel.add(Dense(1, activation='sigmoid'))\n",
    "\t# compile model\n",
    "\topt = Adam(learning_rate=0.0002, beta_1=0.5)\n",
    "\tmodel.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\treturn model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ae97fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 128, 128, 128)     3584      \n",
      "                                                                 \n",
      " leaky_re_lu (LeakyReLU)     (None, 128, 128, 128)     0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 64, 64, 128)       147584    \n",
      "                                                                 \n",
      " leaky_re_lu_1 (LeakyReLU)   (None, 64, 64, 128)       0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 32, 32, 128)       147584    \n",
      "                                                                 \n",
      " leaky_re_lu_2 (LeakyReLU)   (None, 32, 32, 128)       0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 16, 16, 128)       147584    \n",
      "                                                                 \n",
      " leaky_re_lu_3 (LeakyReLU)   (None, 16, 16, 128)       0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 8, 8, 128)         147584    \n",
      "                                                                 \n",
      " leaky_re_lu_4 (LeakyReLU)   (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 8192)              0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 8192)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 8193      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 602113 (2.30 MB)\n",
      "Trainable params: 602113 (2.30 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Verify the model summary\n",
    "test_discr = define_discriminator()\n",
    "print(test_discr.summary())\n",
    "plot_model(test_discr, to_file='disc_model.png', show_shapes=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f36f254",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define the standalone generator model\n",
    "# Generator must generate 128x128x3 images that can be fed into the discriminator. \n",
    "# So, we start with enough nodes in the dense layer that can be gradually upscaled\n",
    "#to 128x128x3. \n",
    "#Remember that the input would be a latent vector (usually size 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94df8677",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_generator(latent_dim):\n",
    "\tmodel = Sequential()\n",
    "\t# Define number of nodes that can be gradually reshaped and upscaled to 128x128x3\n",
    "\tn_nodes = 128 * 8 * 8 #8192 nodes\n",
    "\tmodel.add(Dense(n_nodes, input_dim=latent_dim))\n",
    "\tmodel.add(LeakyReLU(alpha=0.2))\n",
    "\tmodel.add(Reshape((8, 8, 128)))\n",
    "\t# upsample to 16x16\n",
    "\tmodel.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
    "\tmodel.add(LeakyReLU(alpha=0.2))\n",
    "\t# upsample to 32x32\n",
    "\tmodel.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
    "\tmodel.add(LeakyReLU(alpha=0.2))\n",
    "\t# upsample to 64x64\n",
    "\tmodel.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
    "\tmodel.add(LeakyReLU(alpha=0.2))\n",
    "\t# upsample to 128x128\n",
    "\tmodel.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
    "\tmodel.add(LeakyReLU(alpha=0.2))\n",
    "\t# output layer 128x128x3\n",
    "\tmodel.add(Conv2D(3, (8,8), activation='tanh', padding='same')) #tanh goes from [-1,1]\n",
    "\treturn model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d94d2b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_1 (Dense)             (None, 8192)              827392    \n",
      "                                                                 \n",
      " leaky_re_lu_5 (LeakyReLU)   (None, 8192)              0         \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " conv2d_transpose (Conv2DTr  (None, 16, 16, 128)       262272    \n",
      " anspose)                                                        \n",
      "                                                                 \n",
      " leaky_re_lu_6 (LeakyReLU)   (None, 16, 16, 128)       0         \n",
      "                                                                 \n",
      " conv2d_transpose_1 (Conv2D  (None, 32, 32, 128)       262272    \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " leaky_re_lu_7 (LeakyReLU)   (None, 32, 32, 128)       0         \n",
      "                                                                 \n",
      " conv2d_transpose_2 (Conv2D  (None, 64, 64, 128)       262272    \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " leaky_re_lu_8 (LeakyReLU)   (None, 64, 64, 128)       0         \n",
      "                                                                 \n",
      " conv2d_transpose_3 (Conv2D  (None, 128, 128, 128)     262272    \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " leaky_re_lu_9 (LeakyReLU)   (None, 128, 128, 128)     0         \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 128, 128, 3)       24579     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1901059 (7.25 MB)\n",
      "Trainable params: 1901059 (7.25 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_gen = define_generator(100)\n",
    "print(test_gen.summary())\n",
    "plot_model(test_gen, to_file='generator_model.png', show_shapes=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d59e4e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " sequential_1 (Sequential)   (None, 128, 128, 3)       1901059   \n",
      "                                                                 \n",
      " sequential (Sequential)     (None, 1)                 602113    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2503172 (9.55 MB)\n",
      "Trainable params: 1901059 (7.25 MB)\n",
      "Non-trainable params: 602113 (2.30 MB)\n",
      "_________________________________________________________________\n",
      "None\n",
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# define the combined generator and discriminator model, for updating the generator\n",
    "def define_gan(g_model, d_model):\n",
    "\t# make weights in the discriminator not trainable\n",
    "\td_model.trainable = False\n",
    "\t# connect them\n",
    "\tmodel = Sequential()\n",
    "\t# add generator\n",
    "\tmodel.add(g_model)\n",
    "\t# add the discriminator\n",
    "\tmodel.add(d_model)\n",
    "\t# compile model\n",
    "\topt = Adam(learning_rate=0.0002, beta_1=0.5)\n",
    "\tmodel.compile(loss='binary_crossentropy', optimizer=opt)\n",
    "\treturn model\n",
    "\n",
    "test_gan = define_gan(test_gen, test_discr)\n",
    "print(test_gan.summary())\n",
    "plot_model(test_gan, to_file='combined_model.png', show_shapes=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb491b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to sample some random real images\n",
    "def generate_real_samples(dataset, n_samples):\n",
    "\tix = randint(0, dataset.shape[0], n_samples)\n",
    "\tX = dataset[ix]\n",
    "\ty = ones((n_samples, 1)) # Class labels for real images are 1\n",
    "\treturn X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e3381dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to generate random latent points\n",
    "def generate_latent_points(latent_dim, n_samples):\n",
    "\tx_input = randn(latent_dim * n_samples)\n",
    "\tx_input = x_input.reshape(n_samples, latent_dim) #Reshape to be provided as input to the generator. \n",
    "\treturn x_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3829c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate fake images using latent vectors\n",
    "def generate_fake_samples(g_model, latent_dim, n_samples):\n",
    "\tx_input = generate_latent_points(latent_dim, n_samples) #Generate latent points as input to the generator\n",
    "\tX = g_model.predict(x_input) #Use the generator to generate fake images\n",
    "\ty = zeros((n_samples, 1)) # Class labels for fake images are 0\n",
    "\treturn X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2449c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save Plots after every n number of epochs\n",
    "def save_plot(examples, epoch, n=100):\n",
    "\t# scale images from [-1,1] to [0,1] so we can plot\n",
    "\texamples = (examples + 1) / 2.0\n",
    "\tfor i in range(n * n):\n",
    "\t\tplt.subplot(n, n, 1 + i)\n",
    "\t\tplt.axis('off')\n",
    "\t\tplt.imshow(examples[i])\n",
    "\t# save plot to a file so we can view how generated images evolved over epochs\n",
    "\tfilename = 'saved_data_during_training/images/generated_plot_128x128_e%03d.png' % (epoch+1)\n",
    "\tplt.savefig(filename)\n",
    "\tplt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5d512b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to summarize performance periodically. \n",
    "# \n",
    "def summarize_performance(epoch, g_model, d_model, dataset, latent_dim, n_samples=100):\n",
    "\t# Fetch real images\n",
    "\tX_real, y_real = generate_real_samples(dataset, n_samples)\n",
    "\t# evaluate discriminator on real images - get accuracy\n",
    "\t_, acc_real = d_model.evaluate(X_real, y_real, verbose=0)\n",
    "\t# Generate fake images\n",
    "\tx_fake, y_fake = generate_fake_samples(g_model, latent_dim, n_samples)\n",
    "\t# evaluate discriminator on fake images - get accuracy\n",
    "\t_, acc_fake = d_model.evaluate(x_fake, y_fake, verbose=0)\n",
    "\t# Print discriminate accuracies on ral and fake images. \n",
    "\tprint('>Accuracy real: %.0f%%, fake: %.0f%%' % (acc_real*100, acc_fake*100))\n",
    "\t# save generated images periodically using the save_plot function\n",
    "\tsave_plot(x_fake, epoch)\n",
    "\t# save the generator model\n",
    "\tfilename = 'saved_data_during_training/models/generator_model_128x128_%03d.h5' % (epoch+1)\n",
    "\tg_model.save(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4f880563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the generator and discriminator by enumerating batches and epochs. \n",
    "#\n",
    "def train(g_model, d_model, gan_model, dataset, latent_dim, n_epochs=1, n_batch=128):\n",
    "\tbat_per_epo = int(dataset.shape[0] / n_batch)\n",
    "\thalf_batch = int(n_batch / 2) #Disc. trained on half batch real and half batch fake images\n",
    "\t#  enumerate epochs\n",
    "\tfor i in range(n_epochs):\n",
    "\t\t# enumerate batches \n",
    "\t\tfor j in range(bat_per_epo):\n",
    "\t\t\t# Fetch random 'real' images\n",
    "\t\t\tX_real, y_real = generate_real_samples(dataset, half_batch)\n",
    "\t\t\t# Train the discriminator using real images\n",
    "\t\t\td_loss1, _ = d_model.train_on_batch(X_real, y_real)\n",
    "\t\t\t# generate 'fake' images \n",
    "\t\t\tX_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n",
    "\t\t\t# Train the discriminator using fake images\n",
    "\t\t\td_loss2, _ = d_model.train_on_batch(X_fake, y_fake)\n",
    "\t\t\t# Generate latent vectors as input for the generator\n",
    "\t\t\tX_gan = generate_latent_points(latent_dim, n_batch)\n",
    "\t\t\t# Label generated (fake) mages as 1 to fool the discriminator \n",
    "\t\t\ty_gan = ones((n_batch, 1))\n",
    "\t\t\t# Train the generator (via the discriminator's error)\n",
    "\t\t\tg_loss = gan_model.train_on_batch(X_gan, y_gan)\n",
    "\t\t\t# Report disc. and gen losses. \n",
    "\t\t\tprint('Epoch>%d, %d/%d, d1=%.3f, d2=%.3f g=%.3f' %\n",
    "\t\t\t\t(i+1, j+1, bat_per_epo, d_loss1, d_loss2, g_loss))\n",
    "\t\t# evaluate the model performance, sometimes\n",
    "\t\tif (i+1) % 10 == 0:\n",
    "\t\t\tsummarize_performance(i, g_model, d_model, dataset, latent_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0389df4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Now that we defined all necessary functions, let us load data and train the GAN.\n",
    "# Dataset from: https://susanqq.github.io/UTKFace/\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4dbff762",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n=200 #Number of images to read from the directory. (For training)\n",
    "SIZE = 128 #Resize images to this size\n",
    "all_img_list = os.listdir('output_faces') #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e4bd45b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset_list = random.sample(all_img_list, n) #Get n random images from the directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "302ad916",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "SIZE = 128  # Specify the desired size\n",
    "output_folder = 'output_faces'  # Specify the directory where your images are located\n",
    "\n",
    "# Get a list of image files in the \"output_faces\" directory\n",
    "image_files = os.listdir(output_folder)\n",
    "\n",
    "dataset = []\n",
    "\n",
    "for img in image_files:\n",
    "    img_path = os.path.join(output_folder, img)\n",
    "    \n",
    "    # Load the image\n",
    "    temp_img = cv2.imread(img_path)\n",
    "    \n",
    "    if temp_img is not None:\n",
    "        # Convert BGR to RGB\n",
    "        temp_img = cv2.cvtColor(temp_img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Create a PIL Image from the NumPy array\n",
    "        temp_img = Image.fromarray(temp_img)\n",
    "        \n",
    "        # Resize the image\n",
    "        temp_img = temp_img.resize((SIZE, SIZE))\n",
    "        \n",
    "        # Convert the PIL Image to a NumPy array\n",
    "        temp_img = np.array(temp_img)\n",
    "        \n",
    "        # Normalize the pixel values to the range [-1, 1]\n",
    "        temp_img = (temp_img.astype(np.float32) - 127.5) / 127.5\n",
    "        \n",
    "        dataset.append(temp_img)\n",
    "    else:\n",
    "        print(f\"Failed to load image: {img_path}\")\n",
    "\n",
    "# Now 'dataset' contains the preprocessed and normalized images from the \"output_faces\" directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e0c589b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = np.array(dataset) #Convert the list to numpy array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dbc4289f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Rescale to [-1, 1] - remember that the generator uses tanh activation that goes from -1,1\n",
    "dataset = dataset.astype('float32')\n",
    "\t# scale from [0,255] to [-1,1]\n",
    "dataset = (dataset - 127.5) / 127.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "34de99a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# size of the latent space\n",
    "latent_dim = 100\n",
    "# create the discriminator using our pre-defined function\n",
    "d_model = define_discriminator()\n",
    "# create the generator using our pre-defined function\n",
    "g_model = define_generator(latent_dim)\n",
    "# create the gan  using our pre-defined function\n",
    "gan_model = define_gan(g_model, d_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a66b49e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 4s 2s/step\n",
      "Epoch>1, 1/8, d1=0.681, d2=0.695 g=0.692\n",
      "2/2 [==============================] - 3s 2s/step\n",
      "Epoch>1, 2/8, d1=0.472, d2=0.695 g=0.693\n",
      "2/2 [==============================] - 4s 2s/step\n"
     ]
    }
   ],
   "source": [
    "train(g_model, d_model, gan_model, dataset, latent_dim, n_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9100f11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
